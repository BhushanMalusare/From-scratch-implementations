{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:57:44.920359Z",
     "iopub.status.busy": "2024-11-15T11:57:44.920027Z",
     "iopub.status.idle": "2024-11-15T11:57:51.045155Z",
     "shell.execute_reply": "2024-11-15T11:57:51.044193Z",
     "shell.execute_reply.started": "2024-11-15T11:57:44.920324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data import Field\n",
    "from torchtext.data.dataset import TabularDataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torchtext.data.iterator import BucketIterator\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:57:51.047069Z",
     "iopub.status.busy": "2024-11-15T11:57:51.046467Z",
     "iopub.status.idle": "2024-11-15T11:57:51.055069Z",
     "shell.execute_reply": "2024-11-15T11:57:51.054258Z",
     "shell.execute_reply.started": "2024-11-15T11:57:51.047030Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:57:51.057849Z",
     "iopub.status.busy": "2024-11-15T11:57:51.057562Z",
     "iopub.status.idle": "2024-11-15T11:57:51.081527Z",
     "shell.execute_reply": "2024-11-15T11:57:51.080632Z",
     "shell.execute_reply.started": "2024-11-15T11:57:51.057817Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "\n",
    "class Tokenizer:\n",
    "    '''class for tokenizer'''\n",
    "\n",
    "    def __init__(self, text=None):\n",
    "        if text is not None:\n",
    "            self.text = text  # Removed .decode('utf-8') as it's unnecessary in Python 3\n",
    "            self.clean_text()\n",
    "        else:\n",
    "            self.text = None\n",
    "        self.sentences = []\n",
    "        self.tokens = []\n",
    "        self.stemmed_word = []\n",
    "        self.final_list = []\n",
    "\n",
    "    def read_from_file(self, filename):\n",
    "        f = codecs.open(filename, encoding='utf-8')\n",
    "        self.text = f.read()\n",
    "        self.clean_text()\n",
    "\n",
    "    def generate_sentences(self):\n",
    "        '''generates a list of sentences'''\n",
    "        text = self.text\n",
    "        self.sentences = text.split(u\"।\")\n",
    "\n",
    "    def clean_text(self):\n",
    "        '''cleans up the text by removing unwanted characters'''\n",
    "        text = self.text\n",
    "        text = re.sub(r'(\\d+)', r'', text)\n",
    "        text = text.replace(u',', '')\n",
    "        text = text.replace(u'\"', '')\n",
    "        text = text.replace(u'(', '')\n",
    "        text = text.replace(u')', '')\n",
    "        text = text.replace(u':', '')\n",
    "        text = text.replace(u\"'\", '')\n",
    "        text = text.replace(u\"‘‘\", '')\n",
    "        text = text.replace(u\"’’\", '')\n",
    "        text = text.replace(u\"''\", '')\n",
    "        text = text.replace(u\".\", '')\n",
    "        self.text = text\n",
    "\n",
    "    def remove_only_space_words(self):\n",
    "        self.tokens = list(filter(lambda tok: tok.strip(), self.tokens))\n",
    "\n",
    "    def hyphenated_tokens(self):\n",
    "        for each in self.tokens:\n",
    "            if '-' in each:\n",
    "                tok = each.split('-')\n",
    "                self.tokens.remove(each)\n",
    "                self.tokens.extend(tok)\n",
    "\n",
    "    def tokenize(self):\n",
    "        '''tokenizes the text into words'''\n",
    "        if not self.sentences:\n",
    "            self.generate_sentences()\n",
    "\n",
    "        sentences_list = self.sentences\n",
    "        tokens = []\n",
    "        for each in sentences_list:\n",
    "            word_list = each.split(' ')\n",
    "            tokens += word_list\n",
    "        self.tokens = tokens\n",
    "        self.remove_only_space_words()\n",
    "        self.hyphenated_tokens()\n",
    "\n",
    "    def tokens_count(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def sentence_count(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def len_text(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def concordance(self, word):\n",
    "        if not self.sentences:\n",
    "            self.generate_sentences()\n",
    "        concordance_sent = [each for each in self.sentences if word in each]\n",
    "        return concordance_sent\n",
    "\n",
    "    def generate_freq_dict(self):\n",
    "        freq = {}\n",
    "        if not self.tokens:\n",
    "            self.tokenize()\n",
    "        for each in self.tokens:\n",
    "            freq[each] = freq.get(each, 0) + 1\n",
    "        return freq\n",
    "\n",
    "    def generate_stem_words(self, word):\n",
    "        suffixes = {\n",
    "            1: [u\"ो\", u\"े\", u\"ू\", u\"ु\", u\"ी\", u\"ि\", u\"ा\"],\n",
    "            2: [u\"कर\", u\"ाओ\", u\"िए\", u\"ाई\", u\"ाए\", u\"ने\", u\"नी\", u\"ना\", u\"ते\", u\"ीं\", u\"ती\", u\"ता\", u\"ाँ\", u\"ां\", u\"ों\", u\"ें\"],\n",
    "            3: [u\"ाकर\", u\"ाइए\", u\"ाईं\", u\"ाया\", u\"ेगी\", u\"ेगा\", u\"ोगी\", u\"ोगे\", u\"ाने\", u\"ाना\", u\"ाते\", u\"ाती\", u\"ाता\", u\"तीं\", u\"ाओं\", u\"ाएं\", u\"ुओं\", u\"ुएं\", u\"ुआं\"],\n",
    "            4: [u\"ाएगी\", u\"ाएगा\", u\"ाओगी\", u\"ाओगे\", u\"एंगी\", u\"ेंगी\", u\"एंगे\", u\"ेंगे\", u\"ूंगी\", u\"ूंगा\", u\"ातीं\", u\"नाओं\", u\"नाएं\", u\"ताओं\", u\"ताएं\", u\"ियाँ\", u\"ियों\", u\"ियां\"],\n",
    "            5: [u\"ाएंगी\", u\"ाएंगे\", u\"ाऊंगी\", u\"ाऊंगा\", u\"ाइयाँ\", u\"ाइयों\", u\"ाइयां\"],\n",
    "        }\n",
    "        for L in 5, 4, 3, 2, 1:\n",
    "            if len(word) > L + 1:\n",
    "                for suf in suffixes[L]:\n",
    "                    if word.endswith(suf):\n",
    "                        return word[:-L]\n",
    "        return word\n",
    "\n",
    "    def generate_stem_dict(self):\n",
    "        stem_word = {}\n",
    "        if not self.tokens:\n",
    "            self.tokenize()\n",
    "        for each_token in self.tokens:\n",
    "            temp = self.generate_stem_words(each_token)\n",
    "            stem_word[each_token] = temp\n",
    "            self.stemmed_word.append(temp)\n",
    "        return stem_word\n",
    "\n",
    "    def remove_stop_words(self):\n",
    "        f = codecs.open(\"rss.txt\", encoding='utf-8')\n",
    "        if not self.stemmed_word:\n",
    "            self.generate_stem_dict()\n",
    "        stopwords = [x.strip() for x in f.readlines()]\n",
    "        tokens = [i for i in self.stemmed_word if i not in stopwords]\n",
    "        self.final_tokens = tokens\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:57:51.082942Z",
     "iopub.status.busy": "2024-11-15T11:57:51.082611Z",
     "iopub.status.idle": "2024-11-15T11:57:51.090265Z",
     "shell.execute_reply": "2024-11-15T11:57:51.089362Z",
     "shell.execute_reply.started": "2024-11-15T11:57:51.082909Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def hindi_tokenizer(text):\n",
    "    t=Tokenizer(text)\n",
    "    t.tokenize()\n",
    "    return t.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:57:51.091750Z",
     "iopub.status.busy": "2024-11-15T11:57:51.091437Z",
     "iopub.status.idle": "2024-11-15T11:57:52.239120Z",
     "shell.execute_reply": "2024-11-15T11:57:52.238355Z",
     "shell.execute_reply.started": "2024-11-15T11:57:51.091719Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:57:52.240552Z",
     "iopub.status.busy": "2024-11-15T11:57:52.240204Z",
     "iopub.status.idle": "2024-11-15T11:57:52.245147Z",
     "shell.execute_reply": "2024-11-15T11:57:52.244146Z",
     "shell.execute_reply.started": "2024-11-15T11:57:52.240519Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def english_tokenizer(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:57:52.246992Z",
     "iopub.status.busy": "2024-11-15T11:57:52.246448Z",
     "iopub.status.idle": "2024-11-15T11:57:52.254194Z",
     "shell.execute_reply": "2024-11-15T11:57:52.253337Z",
     "shell.execute_reply.started": "2024-11-15T11:57:52.246948Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "english = Field(sequential=True, use_vocab=True, lower=True, tokenize=english_tokenizer)\n",
    "hindi = Field(sequential=True, use_vocab=True, tokenize=hindi_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:57:52.255744Z",
     "iopub.status.busy": "2024-11-15T11:57:52.255405Z",
     "iopub.status.idle": "2024-11-15T11:57:52.261774Z",
     "shell.execute_reply": "2024-11-15T11:57:52.260922Z",
     "shell.execute_reply.started": "2024-11-15T11:57:52.255711Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fields = {'hindi': ('hin', hindi), 'english': ('eng', english)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:57:52.265772Z",
     "iopub.status.busy": "2024-11-15T11:57:52.265409Z",
     "iopub.status.idle": "2024-11-15T12:01:11.351136Z",
     "shell.execute_reply": "2024-11-15T12:01:11.350291Z",
     "shell.execute_reply.started": "2024-11-15T11:57:52.265739Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = TabularDataset.splits(\n",
    "    path = 'B:\\Pytorch\\From_Scratch_Implementations\\hin_to_eng_dataset',\n",
    "    train='hin_eng_train.json',\n",
    "    validation='hin_eng_val.json',\n",
    "    test='hin_eng_test.json',\n",
    "    format='json',\n",
    "    fields=fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:01:11.352611Z",
     "iopub.status.busy": "2024-11-15T12:01:11.352300Z",
     "iopub.status.idle": "2024-11-15T12:01:11.357818Z",
     "shell.execute_reply": "2024-11-15T12:01:11.356898Z",
     "shell.execute_reply.started": "2024-11-15T12:01:11.352578Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hin': ['सलीक़ा'], 'eng': ['benignancy']}\n",
      "{'hin': ['वह', 'स्थिति', 'जिसमें', 'वित्त', 'सम्बन्धी', 'दशाओं', 'अथवा', 'वित्त', 'का', 'प्रबन्धन', 'किया', 'जा', 'रहा', 'हो'], 'eng': ['the', 'way', 'in', 'which', 'finances', 'are', 'placed', 'or', 'arranged', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data[0]))\n",
    "print(vars(test_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:01:11.359163Z",
     "iopub.status.busy": "2024-11-15T12:01:11.358907Z",
     "iopub.status.idle": "2024-11-15T12:01:11.572399Z",
     "shell.execute_reply": "2024-11-15T12:01:11.571635Z",
     "shell.execute_reply.started": "2024-11-15T12:01:11.359133Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "english.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "hindi.build_vocab(train_data, max_size=10000, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:01:11.573813Z",
     "iopub.status.busy": "2024-11-15T12:01:11.573522Z",
     "iopub.status.idle": "2024-11-15T12:01:11.579203Z",
     "shell.execute_reply": "2024-11-15T12:01:11.578201Z",
     "shell.execute_reply.started": "2024-11-15T12:01:11.573781Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_iterator, val_iterator, test_iterator = BucketIterator.splits((train_data, validation_data, test_data),batch_size=12, sort_within_batch=True, sort_key = lambda x: len(x.hin), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:01:11.580503Z",
     "iopub.status.busy": "2024-11-15T12:01:11.580196Z",
     "iopub.status.idle": "2024-11-15T12:01:11.587698Z",
     "shell.execute_reply": "2024-11-15T12:01:11.586926Z",
     "shell.execute_reply.started": "2024-11-15T12:01:11.580471Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for idx, batch in enumerate(train_iterator):\n",
    "#     if idx==4:\n",
    "#         break\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:01:11.589249Z",
     "iopub.status.busy": "2024-11-15T12:01:11.588866Z",
     "iopub.status.idle": "2024-11-15T12:01:11.605556Z",
     "shell.execute_reply": "2024-11-15T12:01:11.604747Z",
     "shell.execute_reply.started": "2024-11-15T12:01:11.589182Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, drop_p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional=True, dropout=drop_p)\n",
    "\n",
    "        self.fc_hidden = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc_cell = nn.Linear(hidden_size*2, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape-->(seq_len, N)\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding.shape--> (seq_len, N, embedding_size)\n",
    "        encoder_states, (hidden, cell) = self.rnn(embedding)\n",
    "        # encoder_states: (seq_len, batch_size, hidden_size*2)\n",
    "        # hidden shape: (1, N, hidden_size)\n",
    "        hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n",
    "        cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\n",
    "        return encoder_states, hidden, cell\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, \n",
    "                 num_layers, drop_p):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(hidden_size*2 + embedding_size, hidden_size, num_layers, dropout=drop_p)\n",
    "        self.energy = nn.Linear(hidden_size*3, 1)\n",
    "        # self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, enocoder_states, hidden, cell):\n",
    "        # shape of x: (N), but we want it to be (1, N) as we process one word at a time\n",
    "        x = x.unsqueeze(0)\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "\n",
    "        sequence_len = enocoder_states.shape[0]\n",
    "        h_reshaped = hidden.repeat(sequence_len, 1, 1)\n",
    "\n",
    "        energy = self.relu(self.energy(torch.cat((h_reshaped, enocoder_states),dim=2)))\n",
    "        attention = self.softmax(energy)\n",
    "        # attention shape --> (seq_length, N, 1)\n",
    "\n",
    "        attention = attention.permute(1, 2, 0)\n",
    "        # attention shape --> (N, 1, seq_length)\n",
    "\n",
    "        enocoder_states = enocoder_states.permute(1, 0, 2)\n",
    "        # encoder shape --> (N, seq_length, hidden_size*2)\n",
    "\n",
    "        context_vector = torch.bmm(attention, enocoder_states).permute(1, 0, 2)\n",
    "\n",
    "        rnn_input = torch.cat((context_vector, embedding), dim=2)\n",
    "\n",
    "        # embedding shape:(1, N, embedding_size)\n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "        # shape of output: (1, N, hidden_size)\n",
    "        predictions = self.fc(output)\n",
    "        # shape of predictions = (1, N, length_of_vocab)\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        return predictions, hidden, cell\n",
    "    \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        # source shape --> (target, N)\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(english.vocab)\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size, requires_grad=True).to(device)\n",
    "        encoder_states, hidden, cell = self.encoder(source)\n",
    "\n",
    "        # Grad start token\n",
    "        x = target[0]\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(x, encoder_states, hidden, cell)\n",
    "\n",
    "            outputs[t] = output\n",
    "\n",
    "            # output--> (N, english_voacb)\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:01:11.607335Z",
     "iopub.status.busy": "2024-11-15T12:01:11.606721Z",
     "iopub.status.idle": "2024-11-15T12:01:11.616743Z",
     "shell.execute_reply": "2024-11-15T12:01:11.615962Z",
     "shell.execute_reply.started": "2024-11-15T12:01:11.607282Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "learning_rates = 3e-4\n",
    "batch_size = 12\n",
    "\n",
    "load_model = False\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size_encoder = len(hindi.vocab)\n",
    "input_size_decoder = len(english.vocab)\n",
    "output_size = len(english.vocab)\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "enc_dropout = 0.3\n",
    "dec_dropout = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:01:11.618031Z",
     "iopub.status.busy": "2024-11-15T12:01:11.617758Z",
     "iopub.status.idle": "2024-11-15T12:01:11.626845Z",
     "shell.execute_reply": "2024-11-15T12:01:11.625865Z",
     "shell.execute_reply.started": "2024-11-15T12:01:11.618000Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfiguration:\u001b[0m\n",
      "Number of Epochs:         \u001b[92m2\u001b[0m\n",
      "Learning Rate:            \u001b[92m0.0003\u001b[0m\n",
      "Batch Size:               \u001b[92m12\u001b[0m\n",
      "Load Model:               \u001b[92mFalse\u001b[0m\n",
      "Device:                   \u001b[92mcuda\u001b[0m\n",
      "Input Size (Encoder):     \u001b[92m10002\u001b[0m\n",
      "Input Size (Decoder):     \u001b[92m10002\u001b[0m\n",
      "Output Size:              \u001b[92m10002\u001b[0m\n",
      "Encoder Embedding Size:   \u001b[92m300\u001b[0m\n",
      "Decoder Embedding Size:   \u001b[92m300\u001b[0m\n",
      "Hidden Size:              \u001b[92m512\u001b[0m\n",
      "Number of Layers:         \u001b[92m1\u001b[0m\n",
      "Encoder Dropout:          \u001b[92m0.3\u001b[0m\n",
      "Decoder Dropout:          \u001b[92m0.3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[1mConfiguration:\\033[0m\")\n",
    "print(f\"{'Number of Epochs:':<25} \\033[92m{num_epochs}\\033[0m\")\n",
    "print(f\"{'Learning Rate:':<25} \\033[92m{learning_rates}\\033[0m\")\n",
    "print(f\"{'Batch Size:':<25} \\033[92m{batch_size}\\033[0m\")\n",
    "print(f\"{'Load Model:':<25} \\033[92m{load_model}\\033[0m\")\n",
    "print(f\"{'Device:':<25} \\033[92m{device}\\033[0m\")\n",
    "print(f\"{'Input Size (Encoder):':<25} \\033[92m{input_size_encoder}\\033[0m\")\n",
    "print(f\"{'Input Size (Decoder):':<25} \\033[92m{input_size_decoder}\\033[0m\")\n",
    "print(f\"{'Output Size:':<25} \\033[92m{output_size}\\033[0m\")\n",
    "print(f\"{'Encoder Embedding Size:':<25} \\033[92m{encoder_embedding_size}\\033[0m\")\n",
    "print(f\"{'Decoder Embedding Size:':<25} \\033[92m{decoder_embedding_size}\\033[0m\")\n",
    "print(f\"{'Hidden Size:':<25} \\033[92m{hidden_size}\\033[0m\")\n",
    "print(f\"{'Number of Layers:':<25} \\033[92m{num_layers}\\033[0m\")\n",
    "print(f\"{'Encoder Dropout:':<25} \\033[92m{enc_dropout}\\033[0m\")\n",
    "print(f\"{'Decoder Dropout:':<25} \\033[92m{dec_dropout}\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:01:11.628111Z",
     "iopub.status.busy": "2024-11-15T12:01:11.627820Z",
     "iopub.status.idle": "2024-11-15T12:01:12.995677Z",
     "shell.execute_reply": "2024-11-15T12:01:12.994662Z",
     "shell.execute_reply.started": "2024-11-15T12:01:11.628078Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\Pytorch\\venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout).to(device)\n",
    "decoder_net = Decoder(input_size_decoder, decoder_embedding_size, hidden_size, output_size, num_layers, dec_dropout).to(device)\n",
    "model = Seq2Seq(encoder_net, decoder_net).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:01:12.997085Z",
     "iopub.status.busy": "2024-11-15T12:01:12.996801Z",
     "iopub.status.idle": "2024-11-15T12:01:13.001631Z",
     "shell.execute_reply": "2024-11-15T12:01:13.000715Z",
     "shell.execute_reply.started": "2024-11-15T12:01:12.997055Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    if param.requires_grad == False:\n",
    "        print(\"Found parameters not requiring gradients!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:01:13.003448Z",
     "iopub.status.busy": "2024-11-15T12:01:13.002996Z",
     "iopub.status.idle": "2024-11-15T12:01:13.012544Z",
     "shell.execute_reply": "2024-11-15T12:01:13.011375Z",
     "shell.execute_reply.started": "2024-11-15T12:01:13.003406Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english.vocab.stoi['<PAD>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:01:13.014348Z",
     "iopub.status.busy": "2024-11-15T12:01:13.014006Z",
     "iopub.status.idle": "2024-11-15T12:01:14.064815Z",
     "shell.execute_reply": "2024-11-15T12:01:14.064027Z",
     "shell.execute_reply.started": "2024-11-15T12:01:13.014316Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pad_idx = english.vocab.stoi['<PAD>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:01:14.066399Z",
     "iopub.status.busy": "2024-11-15T12:01:14.065917Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0 / 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1500/1500 [04:18<00:00,  5.80it/s, loss=5.0447] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   4%|▎         | 18/500 [00:00<00:02, 168.24it/s, loss=0.7582]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch 0: Zero-length input or target.\n",
      "Skipping batch 4: Insufficient output length.\n",
      "Skipping batch 9: Insufficient output length.\n",
      "Skipping batch 19: Insufficient output length.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  16%|█▌        | 79/500 [00:00<00:02, 188.00it/s, loss=3.2650]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch 49: Insufficient output length.\n",
      "Skipping batch 83: Insufficient output length.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 500/500 [00:12<00:00, 39.31it/s, loss=3.9234] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.4674\n",
      "Epoch [1 / 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1500/1500 [04:26<00:00,  5.63it/s, loss=0.9027]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   4%|▍         | 19/500 [00:00<00:02, 169.51it/s, loss=0.5413]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch 0: Zero-length input or target.\n",
      "Skipping batch 4: Insufficient output length.\n",
      "Skipping batch 9: Insufficient output length.\n",
      "Skipping batch 19: Insufficient output length.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  17%|█▋        | 83/500 [00:00<00:02, 195.60it/s, loss=2.1867]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch 49: Insufficient output length.\n",
      "Skipping batch 83: Insufficient output length.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 500/500 [00:13<00:00, 37.54it/s, loss=3.9062] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.4361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch} / {num_epochs}]\")\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Wrap train_iterator with tqdm\n",
    "    train_bar = tqdm(enumerate(train_iterator), \n",
    "                    total=len(train_iterator),\n",
    "                    desc='Training',\n",
    "                    leave=True)\n",
    "    \n",
    "    for batch_idx, batch in train_bar:\n",
    "        input_data = batch.hin.to(device)\n",
    "        target_data = batch.eng.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_data, target_data)\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target_data[1:].reshape(-1)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar description with current loss\n",
    "        train_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_iterator)\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    # Wrap val_iterator with tqdm\n",
    "    val_bar = tqdm(enumerate(val_iterator), \n",
    "                  total=len(val_iterator),\n",
    "                  desc='Validation',\n",
    "                  leave=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in val_bar:\n",
    "            input_data = batch.hin.to(device)\n",
    "            target_data = batch.eng.to(device)\n",
    "            \n",
    "            # Debug: Check for zero-length sequences\n",
    "            if input_data.size(0) == 0 or target_data.size(0) == 0:\n",
    "                print(f\"Skipping batch {batch_idx}: Zero-length input or target.\")\n",
    "                continue\n",
    "\n",
    "            output = model(input_data, target_data, teacher_force_ratio=0)\n",
    "            # Debug: Check for invalid model output\n",
    "            if output.size(0) <= 1:\n",
    "                print(f\"Skipping batch {batch_idx}: Insufficient output length.\")\n",
    "                continue\n",
    "            \n",
    "            output = output[1:].reshape(-1, output.shape[2])\n",
    "            target = target_data[1:].reshape(-1)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            \n",
    "            # Update progress bar description with current loss\n",
    "            val_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_iterator)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Manually clear memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, hindi_tokenizer, hindi_vocab, english_vocab, device, max_length=50):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Tokenize and numericalize the sentence\n",
    "    tokens = hindi_tokenizer(sentence)  # Tokenize Hindi sentence\n",
    "    tokens = [hindi_vocab.stoi[\"<sos>\"]] + [hindi_vocab.stoi.get(token, hindi_vocab.stoi[\"<unk>\"]) for token in tokens] + [hindi_vocab.stoi[\"<eos>\"]]\n",
    "\n",
    "    # Convert to tensor and add batch dimension\n",
    "    source = torch.LongTensor(tokens).unsqueeze(1).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(source)\n",
    "\n",
    "    # Prepare for decoding\n",
    "    outputs = []\n",
    "    x = torch.LongTensor([english_vocab.stoi[\"<sos>\"]]).to(device)  # Start with <SOS> token\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(x, hidden, cell)\n",
    "        \n",
    "        # Get the token with the highest probability\n",
    "        best_guess = output.argmax(1).item()\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        # Break if <EOS> is generated\n",
    "        if best_guess == english_vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "        # Set x to best_guess for next iteration\n",
    "        x = torch.LongTensor([best_guess]).to(device)\n",
    "\n",
    "    # Convert token indices back to words\n",
    "    translated_sentence = [english_vocab.itos[idx] for idx in outputs if idx not in {english_vocab.stoi[\"<sos>\"], english_vocab.stoi[\"<eos>\"]}]\n",
    "    return \" \".join(translated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sentence = \"मेरा नाम भूषण है\"  # Input Hindi sentence as a string\n",
    "translated_sentence = translate_sentence(model, sentence, hindi_tokenizer, hindi.vocab, english.vocab, device)\n",
    "print(translated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage:\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6090184,
     "sourceId": 9911706,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
