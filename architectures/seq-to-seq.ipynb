{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:57:44.920359Z",
     "iopub.status.busy": "2024-11-15T11:57:44.920027Z",
     "iopub.status.idle": "2024-11-15T11:57:51.045155Z",
     "shell.execute_reply": "2024-11-15T11:57:51.044193Z",
     "shell.execute_reply.started": "2024-11-15T11:57:44.920324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data import Field\n",
    "from torchtext.data.dataset import TabularDataset\n",
    "from torchtext.data.iterator import BucketIterator, Iterator\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:57:51.047069Z",
     "iopub.status.busy": "2024-11-15T11:57:51.046467Z",
     "iopub.status.idle": "2024-11-15T11:57:51.055069Z",
     "shell.execute_reply": "2024-11-15T11:57:51.054258Z",
     "shell.execute_reply.started": "2024-11-15T11:57:51.047030Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:57:51.057849Z",
     "iopub.status.busy": "2024-11-15T11:57:51.057562Z",
     "iopub.status.idle": "2024-11-15T11:57:51.081527Z",
     "shell.execute_reply": "2024-11-15T11:57:51.080632Z",
     "shell.execute_reply.started": "2024-11-15T11:57:51.057817Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "\n",
    "class Tokenizer:\n",
    "    '''class for tokenizer'''\n",
    "\n",
    "    def __init__(self, text=None):\n",
    "        if text is not None:\n",
    "            self.text = text  # Removed .decode('utf-8') as it's unnecessary in Python 3\n",
    "            self.clean_text()\n",
    "        else:\n",
    "            self.text = None\n",
    "        self.sentences = []\n",
    "        self.tokens = []\n",
    "        self.stemmed_word = []\n",
    "        self.final_list = []\n",
    "\n",
    "    def read_from_file(self, filename):\n",
    "        f = codecs.open(filename, encoding='utf-8')\n",
    "        self.text = f.read()\n",
    "        self.clean_text()\n",
    "\n",
    "    def generate_sentences(self):\n",
    "        '''generates a list of sentences'''\n",
    "        text = self.text\n",
    "        self.sentences = text.split(u\"।\")\n",
    "\n",
    "    def clean_text(self):\n",
    "        '''cleans up the text by removing unwanted characters'''\n",
    "        text = self.text\n",
    "        text = re.sub(r'(\\d+)', r'', text)\n",
    "        text = text.replace(u',', '')\n",
    "        text = text.replace(u'\"', '')\n",
    "        text = text.replace(u'(', '')\n",
    "        text = text.replace(u')', '')\n",
    "        text = text.replace(u':', '')\n",
    "        text = text.replace(u\"'\", '')\n",
    "        text = text.replace(u\"‘‘\", '')\n",
    "        text = text.replace(u\"’’\", '')\n",
    "        text = text.replace(u\"''\", '')\n",
    "        text = text.replace(u\".\", '')\n",
    "        self.text = text\n",
    "\n",
    "    def remove_only_space_words(self):\n",
    "        self.tokens = list(filter(lambda tok: tok.strip(), self.tokens))\n",
    "\n",
    "    def hyphenated_tokens(self):\n",
    "        for each in self.tokens:\n",
    "            if '-' in each:\n",
    "                tok = each.split('-')\n",
    "                self.tokens.remove(each)\n",
    "                self.tokens.extend(tok)\n",
    "\n",
    "    def tokenize(self):\n",
    "        '''tokenizes the text into words'''\n",
    "        if not self.sentences:\n",
    "            self.generate_sentences()\n",
    "\n",
    "        sentences_list = self.sentences\n",
    "        tokens = []\n",
    "        for each in sentences_list:\n",
    "            word_list = each.split(' ')\n",
    "            tokens += word_list\n",
    "        self.tokens = tokens\n",
    "        self.remove_only_space_words()\n",
    "        self.hyphenated_tokens()\n",
    "\n",
    "    def tokens_count(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def sentence_count(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def len_text(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def concordance(self, word):\n",
    "        if not self.sentences:\n",
    "            self.generate_sentences()\n",
    "        concordance_sent = [each for each in self.sentences if word in each]\n",
    "        return concordance_sent\n",
    "\n",
    "    def generate_freq_dict(self):\n",
    "        freq = {}\n",
    "        if not self.tokens:\n",
    "            self.tokenize()\n",
    "        for each in self.tokens:\n",
    "            freq[each] = freq.get(each, 0) + 1\n",
    "        return freq\n",
    "\n",
    "    def generate_stem_words(self, word):\n",
    "        suffixes = {\n",
    "            1: [u\"ो\", u\"े\", u\"ू\", u\"ु\", u\"ी\", u\"ि\", u\"ा\"],\n",
    "            2: [u\"कर\", u\"ाओ\", u\"िए\", u\"ाई\", u\"ाए\", u\"ने\", u\"नी\", u\"ना\", u\"ते\", u\"ीं\", u\"ती\", u\"ता\", u\"ाँ\", u\"ां\", u\"ों\", u\"ें\"],\n",
    "            3: [u\"ाकर\", u\"ाइए\", u\"ाईं\", u\"ाया\", u\"ेगी\", u\"ेगा\", u\"ोगी\", u\"ोगे\", u\"ाने\", u\"ाना\", u\"ाते\", u\"ाती\", u\"ाता\", u\"तीं\", u\"ाओं\", u\"ाएं\", u\"ुओं\", u\"ुएं\", u\"ुआं\"],\n",
    "            4: [u\"ाएगी\", u\"ाएगा\", u\"ाओगी\", u\"ाओगे\", u\"एंगी\", u\"ेंगी\", u\"एंगे\", u\"ेंगे\", u\"ूंगी\", u\"ूंगा\", u\"ातीं\", u\"नाओं\", u\"नाएं\", u\"ताओं\", u\"ताएं\", u\"ियाँ\", u\"ियों\", u\"ियां\"],\n",
    "            5: [u\"ाएंगी\", u\"ाएंगे\", u\"ाऊंगी\", u\"ाऊंगा\", u\"ाइयाँ\", u\"ाइयों\", u\"ाइयां\"],\n",
    "        }\n",
    "        for L in 5, 4, 3, 2, 1:\n",
    "            if len(word) > L + 1:\n",
    "                for suf in suffixes[L]:\n",
    "                    if word.endswith(suf):\n",
    "                        return word[:-L]\n",
    "        return word\n",
    "\n",
    "    def generate_stem_dict(self):\n",
    "        stem_word = {}\n",
    "        if not self.tokens:\n",
    "            self.tokenize()\n",
    "        for each_token in self.tokens:\n",
    "            temp = self.generate_stem_words(each_token)\n",
    "            stem_word[each_token] = temp\n",
    "            self.stemmed_word.append(temp)\n",
    "        return stem_word\n",
    "\n",
    "    def remove_stop_words(self):\n",
    "        f = codecs.open(\"rss.txt\", encoding='utf-8')\n",
    "        if not self.stemmed_word:\n",
    "            self.generate_stem_dict()\n",
    "        stopwords = [x.strip() for x in f.readlines()]\n",
    "        tokens = [i for i in self.stemmed_word if i not in stopwords]\n",
    "        self.final_tokens = tokens\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:57:51.082942Z",
     "iopub.status.busy": "2024-11-15T11:57:51.082611Z",
     "iopub.status.idle": "2024-11-15T11:57:51.090265Z",
     "shell.execute_reply": "2024-11-15T11:57:51.089362Z",
     "shell.execute_reply.started": "2024-11-15T11:57:51.082909Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def hindi_tokenizer(text):\n",
    "    t=Tokenizer(text)\n",
    "    t.tokenize()\n",
    "    return t.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:57:51.091750Z",
     "iopub.status.busy": "2024-11-15T11:57:51.091437Z",
     "iopub.status.idle": "2024-11-15T11:57:52.239120Z",
     "shell.execute_reply": "2024-11-15T11:57:52.238355Z",
     "shell.execute_reply.started": "2024-11-15T11:57:51.091719Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:57:52.240552Z",
     "iopub.status.busy": "2024-11-15T11:57:52.240204Z",
     "iopub.status.idle": "2024-11-15T11:57:52.245147Z",
     "shell.execute_reply": "2024-11-15T11:57:52.244146Z",
     "shell.execute_reply.started": "2024-11-15T11:57:52.240519Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def english_tokenizer(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:57:52.246992Z",
     "iopub.status.busy": "2024-11-15T11:57:52.246448Z",
     "iopub.status.idle": "2024-11-15T11:57:52.254194Z",
     "shell.execute_reply": "2024-11-15T11:57:52.253337Z",
     "shell.execute_reply.started": "2024-11-15T11:57:52.246948Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "english = Field(sequential=True, use_vocab=True, lower=True, tokenize=english_tokenizer)\n",
    "hindi = Field(sequential=True, use_vocab=True, tokenize=hindi_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:57:52.255744Z",
     "iopub.status.busy": "2024-11-15T11:57:52.255405Z",
     "iopub.status.idle": "2024-11-15T11:57:52.261774Z",
     "shell.execute_reply": "2024-11-15T11:57:52.260922Z",
     "shell.execute_reply.started": "2024-11-15T11:57:52.255711Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fields = {'hindi': ('hin', hindi), 'english': ('eng', english)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:57:52.265772Z",
     "iopub.status.busy": "2024-11-15T11:57:52.265409Z",
     "iopub.status.idle": "2024-11-15T12:01:11.351136Z",
     "shell.execute_reply": "2024-11-15T12:01:11.350291Z",
     "shell.execute_reply.started": "2024-11-15T11:57:52.265739Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = TabularDataset.splits(\n",
    "    path = 'B:\\Pytorch\\From_Scratch_Implementations\\hin_to_eng_dataset',\n",
    "    train='hin_eng_train.json',\n",
    "    validation='hin_eng_val.json',\n",
    "    test='hin_eng_test.json',\n",
    "    format='json',\n",
    "    fields=fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:01:11.352611Z",
     "iopub.status.busy": "2024-11-15T12:01:11.352300Z",
     "iopub.status.idle": "2024-11-15T12:01:11.357818Z",
     "shell.execute_reply": "2024-11-15T12:01:11.356898Z",
     "shell.execute_reply.started": "2024-11-15T12:01:11.352578Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hin': ['सलीक़ा'], 'eng': ['benignancy']}\n",
      "{'hin': ['वह', 'स्थिति', 'जिसमें', 'वित्त', 'सम्बन्धी', 'दशाओं', 'अथवा', 'वित्त', 'का', 'प्रबन्धन', 'किया', 'जा', 'रहा', 'हो'], 'eng': ['the', 'way', 'in', 'which', 'finances', 'are', 'placed', 'or', 'arranged', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data[0]))\n",
    "print(vars(test_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:01:11.359163Z",
     "iopub.status.busy": "2024-11-15T12:01:11.358907Z",
     "iopub.status.idle": "2024-11-15T12:01:11.572399Z",
     "shell.execute_reply": "2024-11-15T12:01:11.571635Z",
     "shell.execute_reply.started": "2024-11-15T12:01:11.359133Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "english.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "hindi.build_vocab(train_data, max_size=10000, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:01:11.573813Z",
     "iopub.status.busy": "2024-11-15T12:01:11.573522Z",
     "iopub.status.idle": "2024-11-15T12:01:11.579203Z",
     "shell.execute_reply": "2024-11-15T12:01:11.578201Z",
     "shell.execute_reply.started": "2024-11-15T12:01:11.573781Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_iterator, val_iterator, test_iterator = BucketIterator.splits((train_data, validation_data, test_data),batch_size=32, sort_within_batch=True, sort_key = lambda x: len(x.hin), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:01:11.580503Z",
     "iopub.status.busy": "2024-11-15T12:01:11.580196Z",
     "iopub.status.idle": "2024-11-15T12:01:11.587698Z",
     "shell.execute_reply": "2024-11-15T12:01:11.586926Z",
     "shell.execute_reply.started": "2024-11-15T12:01:11.580471Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for idx, batch in enumerate(train_iterator):\n",
    "#     if idx==4:\n",
    "#         break\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:01:11.589249Z",
     "iopub.status.busy": "2024-11-15T12:01:11.588866Z",
     "iopub.status.idle": "2024-11-15T12:01:11.605556Z",
     "shell.execute_reply": "2024-11-15T12:01:11.604747Z",
     "shell.execute_reply.started": "2024-11-15T12:01:11.589182Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, drop_p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=drop_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape-->(seq_len, N)\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding.shape--> (seq_len, N, embedding_size)\n",
    "        output, (hidden, cell) = self.rnn(embedding)\n",
    "\n",
    "        return hidden, cell\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, \n",
    "                 num_layers, drop_p):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=drop_p)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # shape of x: (N), but we want it to be (1, N) as we process one word at a time\n",
    "        x = x.unsqueeze(0)\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape:(1, N, embedding_size)\n",
    "        output, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        # shape of output: (1, N, hidden_size)\n",
    "        predictions = self.fc(output)\n",
    "        # shape of predictions = (1, N, length_of_vocab)\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        return predictions, hidden, cell\n",
    "    \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        # source shape --> (target, N)\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(english.vocab)\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size, requires_grad=True).to(device)\n",
    "        hidden, cell = self.encoder(source)\n",
    "\n",
    "        # Grad start token\n",
    "        x = target[0]\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "\n",
    "            outputs[t] = output\n",
    "\n",
    "            # output--> (N, english_voacb)\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:01:11.607335Z",
     "iopub.status.busy": "2024-11-15T12:01:11.606721Z",
     "iopub.status.idle": "2024-11-15T12:01:11.616743Z",
     "shell.execute_reply": "2024-11-15T12:01:11.615962Z",
     "shell.execute_reply.started": "2024-11-15T12:01:11.607282Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "learning_rates = 3e-4\n",
    "batch_size = 32\n",
    "\n",
    "load_model = Fals,e\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size_encoder = len(hindi.vocab)\n",
    "input_size_decoder = len(english.vocab)\n",
    "output_size = len(english.vocab)\n",
    "encoder_embedding_size = 512\n",
    "decoder_embedding_size = 512\n",
    "hidden_size = 1024\n",
    "num_layers = 6\n",
    "enc_dropout = 0.3\n",
    "dec_dropout = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:01:11.618031Z",
     "iopub.status.busy": "2024-11-15T12:01:11.617758Z",
     "iopub.status.idle": "2024-11-15T12:01:11.626845Z",
     "shell.execute_reply": "2024-11-15T12:01:11.625865Z",
     "shell.execute_reply.started": "2024-11-15T12:01:11.618000Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfiguration:\u001b[0m\n",
      "Number of Epochs:         \u001b[92m2\u001b[0m\n",
      "Learning Rate:            \u001b[92m0.0003\u001b[0m\n",
      "Batch Size:               \u001b[92m32\u001b[0m\n",
      "Load Model:               \u001b[92mFalse\u001b[0m\n",
      "Device:                   \u001b[92mcuda\u001b[0m\n",
      "Input Size (Encoder):     \u001b[92m10002\u001b[0m\n",
      "Input Size (Decoder):     \u001b[92m10002\u001b[0m\n",
      "Output Size:              \u001b[92m10002\u001b[0m\n",
      "Encoder Embedding Size:   \u001b[92m512\u001b[0m\n",
      "Decoder Embedding Size:   \u001b[92m512\u001b[0m\n",
      "Hidden Size:              \u001b[92m1024\u001b[0m\n",
      "Number of Layers:         \u001b[92m6\u001b[0m\n",
      "Encoder Dropout:          \u001b[92m0.3\u001b[0m\n",
      "Decoder Dropout:          \u001b[92m0.3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[1mConfiguration:\\033[0m\")\n",
    "print(f\"{'Number of Epochs:':<25} \\033[92m{num_epochs}\\033[0m\")\n",
    "print(f\"{'Learning Rate:':<25} \\033[92m{learning_rates}\\033[0m\")\n",
    "print(f\"{'Batch Size:':<25} \\033[92m{batch_size}\\033[0m\")\n",
    "print(f\"{'Load Model:':<25} \\033[92m{load_model}\\033[0m\")\n",
    "print(f\"{'Device:':<25} \\033[92m{device}\\033[0m\")\n",
    "print(f\"{'Input Size (Encoder):':<25} \\033[92m{input_size_encoder}\\033[0m\")\n",
    "print(f\"{'Input Size (Decoder):':<25} \\033[92m{input_size_decoder}\\033[0m\")\n",
    "print(f\"{'Output Size:':<25} \\033[92m{output_size}\\033[0m\")\n",
    "print(f\"{'Encoder Embedding Size:':<25} \\033[92m{encoder_embedding_size}\\033[0m\")\n",
    "print(f\"{'Decoder Embedding Size:':<25} \\033[92m{decoder_embedding_size}\\033[0m\")\n",
    "print(f\"{'Hidden Size:':<25} \\033[92m{hidden_size}\\033[0m\")\n",
    "print(f\"{'Number of Layers:':<25} \\033[92m{num_layers}\\033[0m\")\n",
    "print(f\"{'Encoder Dropout:':<25} \\033[92m{enc_dropout}\\033[0m\")\n",
    "print(f\"{'Decoder Dropout:':<25} \\033[92m{dec_dropout}\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:01:11.628111Z",
     "iopub.status.busy": "2024-11-15T12:01:11.627820Z",
     "iopub.status.idle": "2024-11-15T12:01:12.995677Z",
     "shell.execute_reply": "2024-11-15T12:01:12.994662Z",
     "shell.execute_reply.started": "2024-11-15T12:01:11.628078Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout).to(device)\n",
    "decoder_net = Decoder(input_size_decoder, decoder_embedding_size, hidden_size, output_size, num_layers, dec_dropout).to(device)\n",
    "model = Seq2Seq(encoder_net, decoder_net).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:01:12.997085Z",
     "iopub.status.busy": "2024-11-15T12:01:12.996801Z",
     "iopub.status.idle": "2024-11-15T12:01:13.001631Z",
     "shell.execute_reply": "2024-11-15T12:01:13.000715Z",
     "shell.execute_reply.started": "2024-11-15T12:01:12.997055Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    if param.requires_grad == False:\n",
    "        print(\"Found parameters not requiring gradients!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:01:13.003448Z",
     "iopub.status.busy": "2024-11-15T12:01:13.002996Z",
     "iopub.status.idle": "2024-11-15T12:01:13.012544Z",
     "shell.execute_reply": "2024-11-15T12:01:13.011375Z",
     "shell.execute_reply.started": "2024-11-15T12:01:13.003406Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english.vocab.stoi['<PAD>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:01:13.014348Z",
     "iopub.status.busy": "2024-11-15T12:01:13.014006Z",
     "iopub.status.idle": "2024-11-15T12:01:14.064815Z",
     "shell.execute_reply": "2024-11-15T12:01:14.064027Z",
     "shell.execute_reply.started": "2024-11-15T12:01:13.014316Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pad_idx = english.vocab.stoi['<PAD>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:01:14.066399Z",
     "iopub.status.busy": "2024-11-15T12:01:14.065917Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0 / 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 563/563 [05:57<00:00,  1.57it/s, loss=4.5144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.1648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 188/188 [00:15<00:00, 11.94it/s, loss=3.8406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.8276\n",
      "Epoch [1 / 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 563/563 [06:09<00:00,  1.52it/s, loss=0.6588] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.9236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 188/188 [00:17<00:00, 10.56it/s, loss=3.6920]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.8070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch} / {num_epochs}]\")\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Wrap train_iterator with tqdm\n",
    "    train_bar = tqdm(enumerate(train_iterator), \n",
    "                    total=len(train_iterator),\n",
    "                    desc='Training',\n",
    "                    leave=True)\n",
    "    \n",
    "    for batch_idx, batch in train_bar:\n",
    "        input_data = batch.hin.to(device).long() \n",
    "        target_data = batch.eng.to(device).long()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_data, target_data)\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target_data[1:].reshape(-1)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar description with current loss\n",
    "        train_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_iterator)\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    # Wrap val_iterator with tqdm\n",
    "    val_bar = tqdm(enumerate(val_iterator), \n",
    "                  total=len(val_iterator),\n",
    "                  desc='Validation',\n",
    "                  leave=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in val_bar:\n",
    "            input_data = batch.hin.to(device).long() \n",
    "            target_data = batch.eng.to(device).long() \n",
    "            output = model(input_data, target_data, teacher_force_ratio=0) \n",
    "            output = output[1:].reshape(-1, output.shape[2])\n",
    "            target = target_data[1:].reshape(-1)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Update progress bar description with current loss\n",
    "            val_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_iterator)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, hindi_tokenizer, hindi_vocab, english_vocab, device, max_length=50):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Tokenize and numericalize the sentence\n",
    "    tokens = hindi_tokenizer(sentence)  # Tokenize Hindi sentence\n",
    "    tokens = [hindi_vocab.stoi[\"<sos>\"]] + [hindi_vocab.stoi.get(token, hindi_vocab.stoi[\"<unk>\"]) for token in tokens] + [hindi_vocab.stoi[\"<eos>\"]]\n",
    "\n",
    "    # Convert to tensor and add batch dimension\n",
    "    source = torch.LongTensor(tokens).unsqueeze(1).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(source)\n",
    "\n",
    "    # Prepare for decoding\n",
    "    outputs = []\n",
    "    x = torch.LongTensor([english_vocab.stoi[\"<sos>\"]]).to(device)  # Start with <SOS> token\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(x, hidden, cell)\n",
    "        \n",
    "        # Get the token with the highest probability\n",
    "        best_guess = output.argmax(1).item()\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        # Break if <EOS> is generated\n",
    "        if best_guess == english_vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "        # Set x to best_guess for next iteration\n",
    "        x = torch.LongTensor([best_guess]).to(device)\n",
    "\n",
    "    # Convert token indices back to words\n",
    "    translated_sentence = [english_vocab.itos[idx] for idx in outputs if idx not in {english_vocab.stoi[\"<sos>\"], english_vocab.stoi[\"<eos>\"]}]\n",
    "    return \" \".join(translated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "sentence = \"मेरा नाम भूषण है\"  # Input Hindi sentence as a string\n",
    "translated_sentence = translate_sentence(model, sentence, hindi_tokenizer, hindi.vocab, english.vocab, device)\n",
    "print(translated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 117,061,394 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage:\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6090184,
     "sourceId": 9911706,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
