{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = {\n",
    "    \"bert_base\": {\n",
    "        \"vocab_size\": 30522,\n",
    "        \"max_seq_len\": 512,\n",
    "        \"d_model\": 768,\n",
    "        \"n_heads\": 12,\n",
    "        \"d_ff\": 3072,\n",
    "        \"encoder_layers\": 12,\n",
    "        \"mha_norm_eps\": 1e-12,\n",
    "        \"ffn_norm_eps\": 1e-12,\n",
    "        \"attn_dropout\": 0.1,\n",
    "        \"ffn_dropout\": 0.1,\n",
    "        \"dropout\": 0.1,\n",
    "        \"pad_token_id\": 0\n",
    "    },\n",
    "    \"bert_large\": {\n",
    "        \"vocab_size\": 30522,\n",
    "        \"max_seq_len\": 512,\n",
    "        \"d_model\": 1024,\n",
    "        \"n_heads\": 16,\n",
    "        \"d_ff\": 4096,\n",
    "        \"encoder_layers\": 24,\n",
    "        \"mha_norm_eps\": 1e-12,\n",
    "        \"ffn_norm_eps\": 1e-12,\n",
    "        \"attn_dropout\": 0.1,\n",
    "        \"ffn_dropout\": 0.1,\n",
    "        \"dropout\": 0.1,\n",
    "        \"pad_token_id\": 0\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.shape[1], :].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        x = self.linear_1(x)\n",
    "        x = self.dropout(F.gelu(x))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_heads, attn_dropout):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.wq = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wk = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wv = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wo = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(p=attn_dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor):\n",
    "        q:torch.Tensor = self.wq(x)\n",
    "        k:torch.Tensor = self.wk(x)\n",
    "        v:torch.Tensor = self.wv(x)\n",
    "\n",
    "        # B,S,H,D -> B,H,S,D\n",
    "        q = q.view(x.shape[0], x.shape[1], self.n_heads, -1).transpose(1, 2)\n",
    "        k = k.view(x.shape[0], x.shape[1], self.n_heads, -1).transpose(1, 2)\n",
    "        v = v.view(x.shape[0], x.shape[1], self.n_heads, -1).transpose(1, 2)\n",
    "\n",
    "\n",
    "        attention_scores:torch.Tensor = q @ k.transpose(-2, -1) / torch.sqrt(torch.tensor(self.d_model))\n",
    "        mask = mask.unsqueeze(1).unsqueeze(2) # B,S -> B,1,1,S\n",
    "        attention_scores = attention_scores.masked_fill(mask==0, -torch.inf)\n",
    "        attention_scores = torch.softmax(attention_scores, dim=-1)\n",
    "        attention_scores = self.dropout(attention_scores) @ v\n",
    "\n",
    "        out = attention_scores.transpose(1,2).contiguous().view(x.shape[0], x.shape[1], -1)\n",
    "        out = self.wo(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, **config):\n",
    "        super().__init__()\n",
    "        self.layer_norm_mha = nn.LayerNorm(config['d_model'], eps=config['mha_norm_eps'])\n",
    "        self.layer_norm_ffn = nn.LayerNorm(config['d_model'], eps=config['ffn_norm_eps'])\n",
    "        self.attention = MultiHeadAttention(config['d_model'], config['n_heads'], config['attn_dropout'])\n",
    "        self.ffn = FeedForwardBlock(config['d_model'], config['d_ff'], config['ffn_dropout'])\n",
    "        self.dropout = nn.Dropout(p=config['dropout'])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x_attn = self.attention(x, mask)\n",
    "        x = x + self.dropout(x_attn)\n",
    "        x = self.layer_norm_mha(x)\n",
    "        x_ffn = self.ffn(x)\n",
    "        x = x + self.dropout(x_ffn)\n",
    "        x = self.layer_norm_ffn(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.embedding = nn.Embedding(self.config['vocab_size'], self.config['d_model'], self.config['pad_token_id'])\n",
    "        self.pe = PositionalEncoding(self.config['d_model'], self.config['max_seq_len'], self.config['dropout'])\n",
    "\n",
    "        self.encoder_stack = nn.ModuleList([Encoder(**config) for _ in range(self.config['encoder_layers'])])\n",
    "        \n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pe(x)\n",
    "\n",
    "        for layer in self.encoder_stack:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel(bert_config['bert_base'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "batch_size, seq_len = 8, 16\n",
    "x = torch.randint(0, 30522, (batch_size, seq_len))\n",
    "mask = torch.ones(batch_size, seq_len)\n",
    "\n",
    "output = model(x, mask)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
